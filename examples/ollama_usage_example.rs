use ollama_rs::{
    Ollama,
    generation::completion::request::GenerationRequest,
};
use tokio;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // ===================================================================
    // 1. åˆå§‹åŒ–å®¢æˆ·ç«¯
    // ===================================================================
    // åŸæœ¬å®ç°ï¼šéœ€è¦æ‰‹åŠ¨é…ç½® URL å’Œè®¤è¯
    // ç®€åŒ–å®ç°ï¼šä½¿ç”¨é»˜è®¤é…ç½®ï¼Œè‡ªåŠ¨è¿æ¥åˆ°æœ¬åœ° Ollama æœåŠ¡
    // è¿™æ˜¯ä¸€ä¸ªç®€åŒ–å®ç°ï¼Œä¸“æ³¨äºæœ¬åœ°å¼€å‘ç¯å¢ƒçš„å¿«é€Ÿè®¾ç½®
    
    // åˆ›å»º Ollama å®¢æˆ·ç«¯å®ä¾‹
    // é»˜è®¤è¿æ¥åˆ° http://localhost:11434
    let ollama = Ollama::default();
    
    // å¦‚æœéœ€è¦è¿æ¥åˆ°è‡ªå®šä¹‰æœåŠ¡å™¨ï¼š
    // let ollama = Ollama::new("http://your-server:11434", None);
    
    println!("âœ… Ollama å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ");
    
    // ===================================================================
    // 2. è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
    // ===================================================================
    println!("\nğŸ“‹ è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨...");
    
    let models = ollama.list_local_models().await?;
    
    println!("å¯ç”¨æ¨¡å‹:");
    for model in models {
        println!("  - {}", model.name);
    }
    
    // ===================================================================
    // 3. å‘é€èŠå¤©å®Œæˆè¯·æ±‚ï¼ˆåŒæ­¥æ¨¡å¼ï¼‰
    // ===================================================================
    println!("\nğŸ’¬ å‘é€åŒæ­¥èŠå¤©å®Œæˆè¯·æ±‚...");
    
    // å‡†å¤‡ç”Ÿæˆè¯·æ±‚
    let request = GenerationRequest::new(
        "llama3.2:1b".to_string(),  // æ¨¡å‹åç§°
        "ä½ å¥½ï¼è¯·ç”¨ä¸­æ–‡ä»‹ç»ä¸€ä¸‹ Rust è¯­è¨€çš„ç‰¹ç‚¹ã€‚".to_string(),  // æç¤ºè¯
    );
    
    // å‘é€è¯·æ±‚å¹¶è·å–å“åº”
    let response = ollama.generate(request).await?;
    
    println!("æ¨¡å‹å“åº”:");
    println!("ğŸ“ {}", response.response);
    if let Some(eval_duration) = response.eval_duration {
        println!("â±ï¸ è€—æ—¶: {:.2} ç§’", eval_duration as f64 / 1_000_000_000.0);
    }
    if let Some(total_duration) = response.total_duration {
        println!("ğŸ”¥ æ€»è€—æ—¶: {:.2} ç§’", total_duration as f64 / 1_000_000_000.0);
    }
    
    // ===================================================================
    // 4. å‘é€èŠå¤©å®Œæˆè¯·æ±‚ï¼ˆæµå¼æ¨¡å¼æ¨¡æ‹Ÿï¼‰
    // ===================================================================
    println!("\nğŸŒŠ å‘é€èŠå¤©å®Œæˆè¯·æ±‚...");
    
    let stream_request = GenerationRequest::new(
        "llama3.2:1b".to_string(),
        "è¯·ç”¨ä¸‰å¥è¯ä»‹ç»ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ã€‚".to_string(),
    );
    
    // æ³¨æ„ï¼šollama-rs 0.3.2 å¯èƒ½æ²¡æœ‰ç›´æ¥çš„æµå¼ API
    // è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨æ™®é€šçš„ generate æ–¹æ³•ä½œä¸ºç¤ºä¾‹
    let response = ollama.generate(stream_request).await?;
    
    println!("æ¨¡å‹å“åº”:");
    println!("ğŸ“ {}", response.response);
    println!("ğŸ’¡ æ³¨æ„ï¼šollama-rs 0.3.2 ç‰ˆæœ¬çš„æµå¼ API å¯èƒ½éœ€è¦ä¸åŒçš„è°ƒç”¨æ–¹å¼");
    
    // ===================================================================
    // 5. é«˜çº§å‚æ•°è®¾ç½®
    // ===================================================================
    println!("\nğŸ”§ é«˜çº§å‚æ•°è®¾ç½®ç¤ºä¾‹...");
    
    let advanced_request = GenerationRequest::new(
        "llama3.2:1b".to_string(),
        "è¯·å¸®æˆ‘å†™ä¸€ä¸ªç®€å•çš„ Rust å‡½æ•°æ¥è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—ã€‚".to_string(),
    );
    
    // æ³¨æ„ï¼šollama-rs 0.3.2 çš„ API å¯èƒ½æ¯”è¾ƒç®€åŒ–
    // é«˜çº§å‚æ•°å¯èƒ½éœ€è¦é€šè¿‡ä¸åŒçš„æ–¹å¼è®¾ç½®
    
    println!("å‘é€é«˜çº§é…ç½®è¯·æ±‚...");
    let advanced_response = ollama.generate(advanced_request).await?;
    
    println!("é«˜çº§é…ç½®å“åº”:");
    println!("ğŸ“ {}", advanced_response.response);
    if let Some(prompt_eval_count) = advanced_response.prompt_eval_count {
        println!("ğŸ“Š ä½¿ç”¨çš„ä»¤ç‰Œæ•°: {}", prompt_eval_count);
    }
    if let Some(eval_count) = advanced_response.eval_count {
        println!("ğŸ“Š ç”Ÿæˆçš„ä»¤ç‰Œæ•°: {}", eval_count);
    }
    println!("ğŸ’¡ æ³¨æ„ï¼šé«˜çº§å‚æ•°è®¾ç½®å¯èƒ½éœ€è¦æŸ¥çœ‹ ollama-rs çš„å…·ä½“æ–‡æ¡£");
    
    // ===================================================================
    // 6. é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶
    // ===================================================================
    println!("\nğŸ›¡ï¸ é”™è¯¯å¤„ç†ç¤ºä¾‹...");
    
    // å°è¯•ä½¿ç”¨ä¸å­˜åœ¨çš„æ¨¡å‹
    let bad_request = GenerationRequest::new(
        "nonexistent-model".to_string(),
        "è¿™ä¸ªè¯·æ±‚ä¼šå¤±è´¥".to_string(),
    );
    
    match ollama.generate(bad_request).await {
        Ok(response) => {
            println!("æ„å¤–æˆåŠŸ: {}", response.response);
        }
        Err(e) => {
            println!("âŒ é¢„æœŸçš„é”™è¯¯: {}", e);
            println!("ğŸ”§ é”™è¯¯å¤„ç†å»ºè®®ï¼šæ£€æŸ¥æ¨¡å‹åç§°ã€ç½‘ç»œè¿æ¥å’Œ Ollama æœåŠ¡çŠ¶æ€");
        }
    }
    
    // ===================================================================
    // 7. å®é™…åº”ç”¨ç¤ºä¾‹ï¼šå¯¹è¯å†å²ç®¡ç†
    // ===================================================================
    println!("\nğŸ’­ å¯¹è¯å†å²ç®¡ç†ç¤ºä¾‹...");
    
    let mut conversation_history = Vec::new();
    
    // ç¬¬ä¸€è½®å¯¹è¯
    let user_message1 = "æˆ‘çš„åå­—æ˜¯å¼ ä¸‰ï¼Œæˆ‘æ˜¯ä¸ªç¨‹åºå‘˜ã€‚";
    conversation_history.push(("user".to_string(), user_message1.to_string()));
    
    let request1 = GenerationRequest::new(
        "llama3.2:1b".to_string(),
        format!("è®°ä½è¿™ä¸ªä¿¡æ¯ï¼š{}", user_message1),
    );
    
    let response1 = ollama.generate(request1).await?;
    conversation_history.push(("assistant".to_string(), response1.response.clone()));
    println!("ç”¨æˆ·: {}", user_message1);
    println!("åŠ©æ‰‹: {}", response1.response);
    
    // ç¬¬äºŒè½®å¯¹è¯
    let user_message2 = "æˆ‘æ˜¯è°ï¼Ÿæˆ‘åšä»€ä¹ˆå·¥ä½œï¼Ÿ";
    conversation_history.push(("user".to_string(), user_message2.to_string()));
    
    // æ„å»ºåŒ…å«å†å²ä¸Šä¸‹æ–‡çš„è¯·æ±‚
    let context = conversation_history
        .iter()
        .map(|(role, content)| format!("{}: {}", role, content))
        .collect::<Vec<_>>()
        .join("\n");
    
    let request2 = GenerationRequest::new(
        "llama3.2:1b".to_string(),
        format!("åŸºäºä»¥ä¸‹å¯¹è¯å†å²ï¼Œå›ç­”é—®é¢˜ï¼š\n\n{}\n\né—®é¢˜ï¼š{}", context, user_message2),
    );
    
    let response2 = ollama.generate(request2).await?;
    conversation_history.push(("assistant".to_string(), response2.response.clone()));
    
    println!("ç”¨æˆ·: {}", user_message2);
    println!("åŠ©æ‰‹: {}", response2.response);
    
    println!("\nâœ… ollama-rs ç”¨æ³•æ¼”ç¤ºå®Œæˆï¼");
    
    Ok(())
}