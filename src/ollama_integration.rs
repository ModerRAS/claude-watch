use ollama_rs::{Ollama, generation::completion::request::GenerationRequest};
use dotenvy::dotenv;
use std::env;

/// åŸæœ¬å®ç°ï¼šç›´æ¥ä¼ é€’å®Œæ•´ URL å­—ç¬¦ä¸²ç»™ Ollama::new
/// ç®€åŒ–å®ç°ï¼šè§£æ URL ä¸ºç‹¬ç«‹çš„ä¸»æœºå’Œç«¯å£å‚æ•°ï¼Œç¬¦åˆ ollama-rs API è¦æ±‚
/// è¿™æ˜¯ä¸€ä¸ªç®€åŒ–å®ç°ï¼Œç§»é™¤äº†å¤æ‚çš„ URL è§£æé€»è¾‘
fn parse_url(url: &str) -> Result<(&str, u16), String> {
    // ç§»é™¤åè®®å‰ç¼€
    let url = url.trim_start_matches("http://").trim_start_matches("https://");
    
    // åˆ†å‰²ä¸»æœºå’Œç«¯å£
    let parts: Vec<&str> = url.split(':').collect();
    match parts.as_slice() {
        [host, port_str] => {
            port_str.parse::<u16>()
                .map(|port| (*host, port))
                .map_err(|_| format!("æ— æ•ˆçš„ç«¯å£å·: {}", port_str))
        }
        [host] => {
            // é»˜è®¤ç«¯å£ 11434
            Ok((*host, 11434))
        }
        _ => Err(format!("æ— æ•ˆçš„ URL æ ¼å¼: {}", url)),
    }
}

#[derive(Debug, PartialEq)]
pub enum TaskStatus {
    Done,
    Stuck,
}

/// åŸæœ¬å®ç°ï¼šä½¿ç”¨ ureq æ‰‹åŠ¨æ„å»º HTTP è¯·æ±‚ï¼Œéœ€è¦æ‰‹åŠ¨å¤„ç† JSON å’Œé”™è¯¯
/// ç®€åŒ–å®ç°ï¼šä½¿ç”¨ ollama-rs åº“æä¾›çš„å°è£…æ¥å£ï¼Œç±»å‹å®‰å…¨ä¸”æ˜“äºç»´æŠ¤
/// è¿™æ˜¯ä¸€ä¸ªç®€åŒ–å®ç°ï¼Œç§»é™¤äº†å¤æ‚çš„ HTTP è¯·æ±‚æ„å»ºå’Œé”™è¯¯å¤„ç†é€»è¾‘
pub async fn ask_ollama_with_ollama_rs(prompt: &str) -> Result<TaskStatus, String> {
    dotenv().ok();
    
    // ä»ç¯å¢ƒå˜é‡è·å–é…ç½®ï¼Œæä¾›é»˜è®¤å€¼
    let ollama_url = env::var("OLLAMA_URL").unwrap_or_else(|_| "http://localhost:11434".to_string());
    let model = env::var("OLLAMA_MODEL").unwrap_or_else(|_| "llama3.2:1b".to_string());
    
    // è§£æ URL è·å–ä¸»æœºå’Œç«¯å£
    let (host, port) = match parse_url(&ollama_url) {
        Ok(result) => result,
        Err(e) => return Err(format!("URL è§£æå¤±è´¥: {}", e)),
    };
    
    // åˆå§‹åŒ– Ollama å®¢æˆ·ç«¯
    let ollama = Ollama::new(host, port);
    
    // æ„å»ºç”Ÿæˆè¯·æ±‚
    let request = GenerationRequest::new(
        model,
        prompt.to_string(),
    );
    // æ³¨æ„ï¼šollama-rs 0.3.2 çš„ API å¯èƒ½æ¯”è¾ƒç®€åŒ–
    // é«˜çº§å‚æ•°å¯èƒ½éœ€è¦é€šè¿‡ä¸åŒçš„æ–¹å¼è®¾ç½®
    
    // å‘é€è¯·æ±‚å¹¶å¤„ç†å“åº”
    match ollama.generate(request).await {
        Ok(response) => {
            let result = response.response.trim();
            match result {
                "DONE" => Ok(TaskStatus::Done),
                "STUCK" => Ok(TaskStatus::Stuck),
                _ => Err(format!("LLM è¿”å›æœªçŸ¥çŠ¶æ€: {}", result)),
            }
        }
        Err(e) => {
            // æä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
            let error_msg = match e.to_string().as_str() {
                s if s.contains("connection refused") => "æ— æ³•è¿æ¥åˆ° Ollama æœåŠ¡ï¼Œè¯·ç¡®ä¿æœåŠ¡æ­£åœ¨è¿è¡Œ",
                s if s.contains("model not found") => "æŒ‡å®šçš„æ¨¡å‹ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥æ¨¡å‹åç§°æˆ–ä½¿ç”¨ 'ollama pull' ä¸‹è½½",
                s if s.contains("timeout") => "è¯·æ±‚è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥",
                _ => "Ollama è°ƒç”¨å¤±è´¥",
            };
            Err(format!("{}: {}", error_msg, e))
        }
    }
}

/// åŸæœ¬å®ç°ï¼šä½¿ç”¨ ureq å’Œ serde_json æ‰‹åŠ¨å¤„ç† HTTP è¯·æ±‚å’Œå“åº”
/// ç®€åŒ–å®ç°ï¼šå°è£…æˆç‹¬ç«‹çš„å‡½æ•°ï¼Œä¾¿äºæµ‹è¯•å’Œé‡ç”¨
/// è¿™æ˜¯ä¸€ä¸ªç®€åŒ–å®ç°ï¼Œä¸“æ³¨äºèŒè´£åˆ†ç¦»å’Œä»£ç å¤ç”¨
pub async fn ask_ollama_simple(prompt: &str, model: &str) -> Result<String, Box<dyn std::error::Error>> {
    let ollama = Ollama::default();
    
    let request = GenerationRequest::new(
        model.to_string(),
        prompt.to_string(),
    );
    
    let response = ollama.generate(request).await?;
    Ok(response.response)
}

/// åŸæœ¬å®ç°ï¼šå¤æ‚çš„åŒæ­¥å¼‚æ­¥æ··åˆä»£ç 
/// ç®€åŒ–å®ç°ï¼šçº¯å¼‚æ­¥å®ç°ï¼Œæ›´ç¬¦åˆç°ä»£ Rust æœ€ä½³å®è·µ
/// è¿™æ˜¯ä¸€ä¸ªç®€åŒ–å®ç°ï¼Œæ¶ˆé™¤äº†åŒæ­¥å¼‚æ­¥æ··åˆçš„å¤æ‚æ€§
pub async fn ask_ollama_streaming(prompt: &str, model: &str) -> Result<String, Box<dyn std::error::Error>> {
    let ollama = Ollama::default();
    
    let request = GenerationRequest::new(
        model.to_string(),
        prompt.to_string(),
    );
    
    // æ³¨æ„ï¼šollama-rs 0.3.2 å¯èƒ½æ²¡æœ‰ç›´æ¥çš„æµå¼ API
    // è¿™é‡Œä½¿ç”¨æ™®é€šçš„ generate æ–¹æ³•ä½œä¸ºç¤ºä¾‹
    let response = ollama.generate(request).await?;
    let full_response = response.response;
    
    println!("{}", full_response);
    Ok(full_response)
}

/// åŸæœ¬å®ç°ï¼šç¡¬ç¼–ç çš„æ¨¡å‹å’Œå‚æ•°
/// ç®€åŒ–å®ç°ï¼šçµæ´»çš„é…ç½®ç³»ç»Ÿï¼Œæ”¯æŒåŠ¨æ€æ¨¡å‹é€‰æ‹©
/// è¿™æ˜¯ä¸€ä¸ªç®€åŒ–å®ç°ï¼Œæä¾›äº†æ›´å¥½çš„å¯é…ç½®æ€§
pub struct OllamaConfig {
    pub url: String,
    pub model: String,
    pub temperature: f32,
    pub max_tokens: u32,
    pub timeout_seconds: u64,
}

impl Default for OllamaConfig {
    fn default() -> Self {
        Self {
            url: "http://localhost:11434".to_string(),
            model: "llama3.2:1b".to_string(),
            temperature: 0.0,
            max_tokens: 4,
            timeout_seconds: 30,
        }
    }
}

impl OllamaConfig {
    pub fn from_env() -> Self {
        dotenv().ok();
        
        Self {
            url: env::var("OLLAMA_URL").unwrap_or_else(|_| "http://localhost:11434".to_string()),
            model: env::var("OLLAMA_MODEL").unwrap_or_else(|_| "llama3.2:1b".to_string()),
            temperature: env::var("OLLAMA_TEMPERATURE")
                .unwrap_or_else(|_| "0.0".to_string())
                .parse()
                .unwrap_or(0.0),
            max_tokens: env::var("OLLAMA_MAX_TOKENS")
                .unwrap_or_else(|_| "4".to_string())
                .parse()
                .unwrap_or(4),
            timeout_seconds: env::var("OLLAMA_TIMEOUT")
                .unwrap_or_else(|_| "30".to_string())
                .parse()
                .unwrap_or(30),
        }
    }
}

/// åŸæœ¬å®ç°ï¼šå•ä¸€åŠŸèƒ½çš„å‡½æ•°
/// ç®€åŒ–å®ç°ï¼šåŸºäºé…ç½®çš„çµæ´»å®¢æˆ·ç«¯
/// è¿™æ˜¯ä¸€ä¸ªç®€åŒ–å®ç°ï¼Œæ”¯æŒå¤šç§ä½¿ç”¨åœºæ™¯
pub struct OllamaClient {
    client: Ollama,
    config: OllamaConfig,
}

impl OllamaClient {
    pub fn new(config: OllamaConfig) -> Result<Self, Box<dyn std::error::Error>> {
        // è§£æ URL è·å–ä¸»æœºå’Œç«¯å£
        let (host, port) = parse_url(&config.url)
            .map_err(|e| format!("URL è§£æå¤±è´¥: {}", e))?;
        
        let client = Ollama::new(host, port);
        Ok(Self { client, config })
    }
    
    pub fn from_env() -> Result<Self, Box<dyn std::error::Error>> {
        let config = OllamaConfig::from_env();
        Self::new(config)
    }
    
    pub async fn ask_status(&self, prompt: &str) -> Result<TaskStatus, String> {
        let request = GenerationRequest::new(
            self.config.model.clone(),
            prompt.to_string(),
        );
        // æ³¨æ„ï¼šé«˜çº§å‚æ•°è®¾ç½®å¯èƒ½éœ€è¦é€šè¿‡ä¸åŒçš„æ–¹å¼å®ç°
        
        match self.client.generate(request).await {
            Ok(response) => {
                let result = response.response.trim();
                match result {
                    "DONE" => Ok(TaskStatus::Done),
                    "STUCK" => Ok(TaskStatus::Stuck),
                    _ => Err(format!("LLM è¿”å›æœªçŸ¥çŠ¶æ€: {}", result)),
                }
            }
            Err(e) => {
                let error_msg = match e.to_string().as_str() {
                    s if s.contains("connection refused") => "æ— æ³•è¿æ¥åˆ° Ollama æœåŠ¡",
                    s if s.contains("model not found") => "æ¨¡å‹ä¸å­˜åœ¨",
                    s if s.contains("timeout") => "è¯·æ±‚è¶…æ—¶",
                    _ => "Ollama è°ƒç”¨å¤±è´¥",
                };
                Err(format!("{}: {}", error_msg, e))
            }
        }
    }
    
    pub async fn ask(&self, prompt: &str) -> Result<String, Box<dyn std::error::Error>> {
        let request = GenerationRequest::new(
            self.config.model.clone(),
            prompt.to_string(),
        );
        // æ³¨æ„ï¼šé«˜çº§å‚æ•°è®¾ç½®å¯èƒ½éœ€è¦é€šè¿‡ä¸åŒçš„æ–¹å¼å®ç°
        
        let response = self.client.generate(request).await?;
        Ok(response.response)
    }
    
    pub async fn ask_streaming(&self, prompt: &str) -> Result<String, Box<dyn std::error::Error>> {
        let request = GenerationRequest::new(
            self.config.model.clone(),
            prompt.to_string(),
        );
        
        // æ³¨æ„ï¼šollama-rs 0.3.2 å¯èƒ½æ²¡æœ‰ç›´æ¥çš„æµå¼ API
        // è¿™é‡Œä½¿ç”¨æ™®é€šçš„ generate æ–¹æ³•ä½œä¸ºç¤ºä¾‹
        let response = self.client.generate(request).await?;
        let full_response = response.response;
        
        println!("{}", full_response);
        Ok(full_response)
    }
}

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    dotenv().ok();
    
    println!("ğŸš€ ollama-rs é›†æˆç¤ºä¾‹");
    
    // ç¤ºä¾‹ 1: åŸºæœ¬çŠ¶æ€æ£€æŸ¥
    println!("\nğŸ“‹ ç¤ºä¾‹ 1: åŸºæœ¬çŠ¶æ€æ£€æŸ¥");
    let prompt = "æ ¹æ®ä»¥ä¸‹å†…å®¹åˆ¤æ–­ä»»åŠ¡çŠ¶æ€ï¼Œåªè¿”å› DONE æˆ– STUCKï¼š\n\nâœ… All checks passed";
    
    match ask_ollama_with_ollama_rs(prompt).await {
        Ok(TaskStatus::Done) => println!("âœ… çŠ¶æ€: DONE"),
        Ok(TaskStatus::Stuck) => println!("âš ï¸ çŠ¶æ€: STUCK"),
        Err(e) => println!("âŒ é”™è¯¯: {}", e),
    }
    
    // ç¤ºä¾‹ 2: ä½¿ç”¨é…ç½®å®¢æˆ·ç«¯
    println!("\nâš™ï¸ ç¤ºä¾‹ 2: ä½¿ç”¨é…ç½®å®¢æˆ·ç«¯");
    let client = OllamaClient::from_env()?;
    
    let chat_prompt = "ä½ å¥½ï¼è¯·ç”¨ä¸€å¥è¯ä»‹ç»ä½ è‡ªå·±ã€‚";
    match client.ask(chat_prompt).await {
        Ok(response) => println!("ğŸ¤– å›ç­”: {}", response),
        Err(e) => println!("âŒ é”™è¯¯: {}", e),
    }
    
    // ç¤ºä¾‹ 3: æµå¼è¾“å‡º
    println!("\nğŸŒŠ ç¤ºä¾‹ 3: æµå¼è¾“å‡º");
    let stream_prompt = "è¯·å†™ä¸€ä¸ªå…³äºç¼–ç¨‹çš„ä¸‰å¥è¯—ï¼š";
    
    println!("æµå¼å“åº”:");
    match client.ask_streaming(stream_prompt).await {
        Ok(_) => println!("\nâœ… æµå¼è¾“å‡ºå®Œæˆ"),
        Err(e) => println!("âŒ é”™è¯¯: {}", e),
    }
    
    Ok(())
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[tokio::test]
    async fn test_ollama_config_from_env() {
        // æµ‹è¯•é…ç½®åŠ è½½
        let config = OllamaConfig::from_env();
        assert!(!config.url.is_empty());
        assert!(!config.model.is_empty());
        assert!(config.temperature >= 0.0 && config.temperature <= 1.0);
        assert!(config.max_tokens > 0);
    }
    
    #[tokio::test]
    async fn test_ollama_client_creation() {
        // æµ‹è¯•å®¢æˆ·ç«¯åˆ›å»º
        let config = OllamaConfig::default();
        let client = OllamaClient::new(config);
        
        // æ³¨æ„ï¼šè¿™ä¸ªæµ‹è¯•å¯èƒ½ä¼šå› ä¸º Ollama æœåŠ¡æœªè¿è¡Œè€Œå¤±è´¥
        // åœ¨å®é™…æµ‹è¯•ä¸­åº”è¯¥ä½¿ç”¨ mock æˆ–è€…è·³è¿‡æ¡ä»¶
        match client {
            Ok(_) => println!("âœ… å®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ"),
            Err(e) => println!("âš ï¸ å®¢æˆ·ç«¯åˆ›å»ºå¤±è´¥ï¼ˆå¯èƒ½æ˜¯å› ä¸º Ollama æœåŠ¡æœªè¿è¡Œï¼‰: {}", e),
        }
    }
}