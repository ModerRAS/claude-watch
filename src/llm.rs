use crate::config::{Config, OpenAiConfig, OpenRouterConfig};
use serde_json::{json, Value};
// tokio imported implicitly through async functions

/// ä»»åŠ¡çŠ¶æ€æšä¸¾
#[derive(Debug, PartialEq)]
pub enum TaskStatus {
    Done,
    Stuck,
}

/// è§£æ Ollama URL ä¸ºä¸»æœºå’Œç«¯å£
fn parse_ollama_url(url: &str) -> (String, u16) {
    // ç§»é™¤åè®®å‰ç¼€
    let url = url.trim_start_matches("http://").trim_start_matches("https://");
    
    // åˆ†å‰²ä¸»æœºå’Œç«¯å£
    let parts: Vec<&str> = url.split(':').collect();
    match parts.as_slice() {
        [host, port_str] => {
            let port = port_str.parse::<u16>().unwrap_or(11434);
            (host.to_string(), port)
        }
        [host] => {
            // é»˜è®¤ç«¯å£ 11434
            (host.to_string(), 11434)
        }
        _ => {
            // é»˜è®¤å€¼
            ("localhost".to_string(), 11434)
        }
    }
}

/// ä½¿ç”¨ ollama-rs åº“è°ƒç”¨ Ollama API
async fn ask_ollama_with_ollama_rs(prompt_text: &str, model: &str, url: &str) -> Result<String, String> {
    // è§£æ URL è·å–ä¸»æœºå’Œç«¯å£
    let (host, port) = parse_ollama_url(url);
    
    // åˆå§‹åŒ– Ollama å®¢æˆ·ç«¯ï¼ˆæ”¯æŒè‡ªå®šä¹‰æœåŠ¡å™¨åœ°å€å’Œç«¯å£ï¼‰
    let ollama = ollama_rs::Ollama::new(&host, port);
    
    // è®¾ç½®æ¨¡å‹é€‰é¡¹ä»¥æé«˜ç¨³å®šæ€§å’Œä¸€è‡´æ€§
    let options = ollama_rs::models::ModelOptions::default()
        .temperature(0.0)  // ç¡®ä¿ç¡®å®šæ€§è¾“å‡º
        .num_predict(4);   // é™åˆ¶è¾“å‡ºé•¿åº¦
    
    // æ„å»ºç”Ÿæˆè¯·æ±‚
    let request = ollama_rs::generation::completion::request::GenerationRequest::new(
        model.to_string(),
        prompt_text.to_string(),
    ).options(options);
    
    // å‘é€è¯·æ±‚å¹¶å¤„ç†å“åº”
    match ollama.generate(request).await {
        Ok(response) => {
            Ok(response.response)
        }
        Err(e) => {
            Err(format!("Ollama è°ƒç”¨å¤±è´¥: {}", e))
        }
    }
}

/// æ‰‹æ“ HTTP è¯·æ±‚è°ƒç”¨ OpenAI å…¼å®¹çš„ API
async fn ask_openai(system_prompt: &str, user_content: &str, config: &crate::config::OpenAiConfig) -> Result<String, String> {
    use serde_json::{json, Value};
    
    // æ£€æŸ¥ API key æ˜¯å¦ä¸ºç©º
    if config.api_key.is_empty() {
        return Err("OpenAI API key æœªè®¾ç½®".to_string());
    }
    
    // åˆ›å»ºè¯·æ±‚ä½“
    let request_body = json!({
        "model": config.model,
        "messages": [
            {
                "role": "system",
                "content": system_prompt
            },
            {
                "role": "user",
                "content": user_content
            }
        ],
        "max_tokens": 4,
        "temperature": 0.0
    });
    
    // æ„å»ºå®Œæ•´çš„ URL
    let url = if config.api_base.ends_with('/') {
        format!("{}chat/completions", config.api_base)
    } else {
        format!("{}/chat/completions", config.api_base)
    };
    
    // å‘é€ HTTP è¯·æ±‚
    let client = reqwest::Client::new();
    let response = client
        .post(&url)
        .header("Authorization", format!("Bearer {}", config.api_key))
        .header("Content-Type", "application/json")
        .json(&request_body)
        .send()
        .await
        .map_err(|e| format!("HTTP è¯·æ±‚å¤±è´¥: {}", e))?;
    
    // æ£€æŸ¥å“åº”çŠ¶æ€
    if !response.status().is_success() {
        let status = response.status();
        let error_text = response.text().await.unwrap_or_else(|_| "æ— æ³•è·å–é”™è¯¯ä¿¡æ¯".to_string());
        return Err(format!("API è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {}, é”™è¯¯: {}", status, error_text));
    }
    
    // è§£æå“åº” JSON
    let response_text = response
        .text()
        .await
        .map_err(|e| format!("è¯»å–å“åº”å¤±è´¥: {}", e))?;
    
    let json_response: Value = serde_json::from_str(&response_text)
        .map_err(|e| format!("è§£æ JSON å¤±è´¥: {}, å“åº”: {}", e, response_text))?;
    
    // æå–ç»“æœ
    if let Some(choices) = json_response.get("choices").and_then(|v| v.as_array()) {
        if let Some(first_choice) = choices.first() {
            if let Some(message) = first_choice.get("message").and_then(|v| v.as_object()) {
                // åªä» content å­—æ®µåˆ¤æ–­ï¼Œå¿½ç•¥æ¨ç†è¿‡ç¨‹
                if let Some(content) = message.get("content").and_then(|v| v.as_str()) {
                    if !content.is_empty() {
                        return Ok(content.to_string());
                    }
                }
                
                // å¦‚æœ content ä¸ºç©ºï¼Œå¿½ç•¥æ¨ç†è¿‡ç¨‹ï¼Œç›´æ¥è¿”å› STUCK
                // å› ä¸ºç”»é¢å·²ç»åœæ­¢å˜åŒ–ï¼Œé»˜è®¤è®¤ä¸ºå¡ä½
                return Ok("STUCK".to_string());
            }
        }
    }
    
    Err("æ— æ³•è§£æ API å“åº”".to_string())
}

/// ç®€åŒ–çš„å¯å‘å¼æ£€æŸ¥ï¼ˆä»…åœ¨ LLM ä¸å¯ç”¨æ—¶ä½¿ç”¨ï¼‰
fn simple_heuristic_check(text: &str) -> TaskStatus {
    // æ£€æŸ¥æ˜æ˜¾çš„å®Œæˆæ ‡å¿—
    let done_patterns = [
        "âœ… All checks passed",
        "Build completed successfully", 
        "Task finished",
        "All tasks completed",
        "ä»»åŠ¡å®Œæˆ",
        "æå®š",
        "å®Œæˆäº†",
        "Finished",
        "Completed",
        "Done",
        "âœ…",
        "å·¥ä½œå·²å®Œæˆ",
        "æ‰€æœ‰æ­¥éª¤å·²å®Œæˆ",
        "ä»£ç ç”Ÿæˆå®Œæ¯•",
        "æ‰€æœ‰æ–‡ä»¶å·²åˆ›å»ºå®Œæˆ",
    ];
    
    if done_patterns.iter().any(|&pattern| text.contains(pattern)) {
        return TaskStatus::Done;
    }
    
    // æ£€æŸ¥æ˜æ˜¾çš„é”™è¯¯æ ‡å¿—
    let error_patterns = [
        "Error:",
        "error:", 
        "Failed",
        "failed",
        "panic!",
        "stack trace",
        "å‡ºé”™",
        "å¤±è´¥",
        "é”™è¯¯",
        "å¡ä½",
        "stuck",
        "timeout",
        "è¶…æ—¶",
        "æ— å“åº”",
        "æ— æ³•ç»§ç»­",
        "ä¸­æ–­",
    ];
    
    if error_patterns.iter().any(|&pattern| text.contains(pattern)) {
        return TaskStatus::Stuck;
    }
    
    // æ£€æŸ¥å¯èƒ½ä»åœ¨å¤„ç†ä¸­çš„çŠ¶æ€ï¼ˆé¿å…è¯¯åˆ¤ä¸ºå¡ä½ï¼‰
    let processing_patterns = [
        "Cogitating",
        "Thinking",
        "Processing",
        "Working",
        "Analyzing",
        "Generating",
        "Compiling",
        "Building",
        "Installing",
        "Tool use",
        "Calling tool",
        "Function call",
        "API call",
        "Reading file",
        "Writing file",
        "Creating file",
        "Editing file",
        "Downloading",
        "Uploading",
        "Checking",
        "Testing",
        "Retry",
        "Escaping",
        "Interrupting",
        "...",
        "â–ªâ–ªâ–ª",
        "â—¦â—¦â—¦",
    ];
    
    // å¦‚æœæ£€æµ‹åˆ°å¤„ç†ä¸­çŠ¶æ€ï¼Œä¸è½»æ˜“åˆ¤æ–­ä¸ºå¡ä½
    // è¿™é‡Œè¿”å›ä¸€ä¸ªç‰¹æ®ŠçŠ¶æ€ï¼Œè®©ç›‘æ§é€»è¾‘ç»§ç»­ç­‰å¾…
    if processing_patterns.iter().any(|&pattern| text.contains(pattern)) {
        // ç®€åŒ–èµ·è§ï¼Œæˆ‘ä»¬ä»ç„¶è¿”å› Stuckï¼Œä½†åœ¨ç›‘æ§é€»è¾‘ä¸­éœ€è¦å¤„ç†è¿™ç§æƒ…å†µ
        // æ›´å¥½çš„åšæ³•æ˜¯å¢åŠ ä¸€ä¸ª Processing çŠ¶æ€ï¼Œä½†ç°åœ¨å…ˆè¿™æ ·å¤„ç†
        return TaskStatus::Stuck;
    }
    
    // æ£€æŸ¥æ˜¯å¦æœ‰æœªå®Œæˆçš„å‘½ä»¤æˆ–ç¨‹åºè¾“å‡º
    let lines: Vec<&str> = text.lines().collect();
    let last_few_lines: Vec<&str> = lines.iter().rev().take(5).cloned().collect();
    let last_content = last_few_lines.join("\n");
    
    // è¿™ä¸ªé€»è¾‘ç§»åˆ°äº†monitor.rsä¸­çš„check_if_should_skip_llm_callå‡½æ•°ä¸­
    // è¿™é‡Œåªå¤„ç†å¯å‘å¼æ£€æŸ¥ï¼Œä¸è·³è¿‡LLMè°ƒç”¨åˆ¤æ–­
    
    // å¦‚æœæœ€åå‡ è¡Œçœ‹èµ·æ¥åƒæ˜¯ç¨‹åºè¾“å‡ºçš„ä¸€éƒ¨åˆ†ï¼Œå¯èƒ½æ˜¯æ­£å¸¸çš„å¤„ç†çŠ¶æ€
    if last_content.contains('$') || last_content.contains('>') || last_content.contains('#') {
        // å¦‚æœæœ‰å‘½ä»¤æç¤ºç¬¦ï¼Œå¯èƒ½æ˜¯åœ¨ç­‰å¾…è¾“å…¥ï¼Œä¸ç®—å¡ä½
        return TaskStatus::Stuck;
    }
    
    // å¦‚æœæ–‡æœ¬çœ‹èµ·æ¥ä¸å®Œæ•´æˆ–è€…æœ‰æœªé—­åˆçš„ç»“æ„ï¼Œå¯èƒ½æ˜¯æ‰§è¡Œä¸­
    if text.ends_with("...") || text.ends_with("â€¢") || text.ends_with("â–ª") {
        // ä»åœ¨å¤„ç†ä¸­ï¼Œä¸ç«‹å³åˆ¤æ–­ä¸ºå¡ä½
        return TaskStatus::Stuck;
    }
    
    // é»˜è®¤è®¤ä¸ºå¡ä½ï¼ˆå› ä¸ºç”»é¢å·²ç»åœæ­¢å˜åŒ–äº†ï¼‰
    // ä½†è¿™ä¸ªé€»è¾‘ç°åœ¨æ›´åŠ è°¨æ…ï¼Œåªæœ‰åœ¨ç¡®è®¤æ²¡æœ‰å…¶ä»–çŠ¶æ€æ—¶æ‰åˆ¤æ–­ä¸ºå¡ä½
    TaskStatus::Stuck
}

/// ä½¿ç”¨ LLM ç”Ÿæˆæ¿€æ´»æ¶ˆæ¯
/// 
/// è¿™æ˜¯æ™ºèƒ½æ¿€æ´»åŠŸèƒ½ï¼Œè®©LLMç”Ÿæˆä¸€å¥è¯æ¥æ¿€æ´»å¡ä½çš„Claude Code
pub async fn ask_llm_for_activation(prompt: &str, backend: &str, config: &Config) -> Result<String, String> {
    match backend {
        "openai" => {
            if let Some(openai_config) = &config.llm.openai {
                ask_openai_for_activation(prompt, openai_config).await
            } else {
                Err("OpenAIé…ç½®æœªæ‰¾åˆ°".to_string())
            }
        },
        "openrouter" => {
            if let Some(openrouter_config) = &config.llm.openrouter {
                ask_openrouter_for_activation(prompt, openrouter_config).await
            } else {
                Err("OpenRouteré…ç½®æœªæ‰¾åˆ°".to_string())
            }
        },
        "ollama" => {
            if let Some(ollama_config) = &config.llm.ollama {
                ask_ollama_with_ollama_rs(prompt, &ollama_config.model, &ollama_config.url).await
            } else {
                Err("Ollamaé…ç½®æœªæ‰¾åˆ°".to_string())
            }
        },
        _ => {
            Err(format!("ä¸æ”¯æŒçš„LLMåç«¯: {}", backend))
        }
    }
}

/// ä½¿ç”¨OpenAIç”Ÿæˆæ¿€æ´»æ¶ˆæ¯
async fn ask_openai_for_activation(prompt: &str, config: &OpenAiConfig) -> Result<String, String> {
    let client = reqwest::Client::new();
    
    // æ„å»ºæ­£ç¡®çš„URL - æ·»åŠ  chat/completions è·¯å¾„
    let url = if config.api_base.ends_with('/') {
        format!("{}chat/completions", config.api_base)
    } else {
        format!("{}/chat/completions", config.api_base)
    };
    
    let request_body = serde_json::json!({
        "model": &config.model,
        "messages": [
            {
                "role": "system",
                "content": "ä½ æ˜¯ä¸€ä¸ªClaude Codeæ¿€æ´»åŠ©æ‰‹ã€‚å½“Claude Codeå¡ä½æ—¶ï¼Œä½ éœ€è¦ç”Ÿæˆä¸€å¥ç®€çŸ­ã€æœ‰æ•ˆçš„è¯æ¥æ¿€æ´»å®ƒã€‚"
            },
            {
                "role": "user", 
                "content": prompt
            }
        ],
        "max_tokens": 50,
        "temperature": 0.1
    });
    
    println!("ğŸ” OpenAIè¯·æ±‚URL: {}", url);
    println!("ğŸ” OpenAIè¯·æ±‚ä½“: {}", request_body);
    
    let response = client
        .post(&url)
        .header("Authorization", format!("Bearer {}", &config.api_key))
        .header("Content-Type", "application/json")
        .json(&request_body)
        .send()
        .await;
    
    match response {
        Ok(resp) => {
            if let Ok(text) = resp.text().await {
                // è°ƒè¯•ï¼šæ‰“å°åŸå§‹å“åº”
                println!("ğŸ” OpenAIåŸå§‹å“åº”: {}", text);
                
                if let Ok(json) = serde_json::from_str::<serde_json::Value>(&text) {
                    println!("ğŸ” OpenAI JSONè§£ææˆåŠŸ: {:?}", json);
                    
                    if let Some(content) = json["choices"][0]["message"]["content"].as_str() {
                        return Ok(content.trim().to_string());
                    } else {
                        println!("ğŸ” OpenAI JSONä¸­æ‰¾ä¸åˆ°contentå­—æ®µ");
                        return Err("OpenAIå“åº”ä¸­ç¼ºå°‘contentå­—æ®µ".to_string());
                    }
                } else {
                    println!("ğŸ” OpenAI JSONè§£æå¤±è´¥ï¼ŒåŸå§‹æ–‡æœ¬: {}", text);
                    return Err(format!("OpenAI JSONè§£æå¤±è´¥ï¼ŒåŸå§‹å“åº”: {}", text));
                }
            } else {
                return Err("OpenAIå“åº”è¯»å–å¤±è´¥".to_string());
            }
        },
        Err(e) => {
            Err(format!("OpenAIè¯·æ±‚å¤±è´¥: {}", e))
        }
    }
}

/// ä½¿ç”¨OpenRouterç”Ÿæˆæ¿€æ´»æ¶ˆæ¯
async fn ask_openrouter_for_activation(prompt: &str, config: &OpenRouterConfig) -> Result<String, String> {
    let client = reqwest::Client::new();
    
    let request_body = serde_json::json!({
        "model": &config.model,
        "messages": [
            {
                "role": "system",
                "content": "ä½ æ˜¯ä¸€ä¸ªClaude Codeæ¿€æ´»åŠ©æ‰‹ã€‚å½“Claude Codeå¡ä½æ—¶ï¼Œä½ éœ€è¦ç”Ÿæˆä¸€å¥ç®€çŸ­ã€æœ‰æ•ˆçš„è¯æ¥æ¿€æ´»å®ƒã€‚"
            },
            {
                "role": "user", 
                "content": prompt
            }
        ],
        "max_tokens": 50,
        "temperature": 0.1
    });
    
    let response = client
        .post("https://openrouter.ai/api/v1/chat/completions")
        .header("Authorization", format!("Bearer {}", &config.api_key))
        .header("Content-Type", "application/json")
        .json(&request_body)
        .send()
        .await;
    
    match response {
        Ok(resp) => {
            if let Ok(text) = resp.text().await {
                if let Ok(json) = serde_json::from_str::<serde_json::Value>(&text) {
                    if let Some(content) = json["choices"][0]["message"]["content"].as_str() {
                        return Ok(content.trim().to_string());
                    }
                }
            }
            Err("OpenRouterå“åº”è§£æå¤±è´¥".to_string())
        },
        Err(e) => {
            Err(format!("OpenRouterè¯·æ±‚å¤±è´¥: {}", e))
        }
    }
}

/// ä½¿ç”¨ LLM åˆ¤æ–­ Claude Code æœ€ç»ˆçŠ¶æ€
/// 
/// è¿™æ˜¯æœ€å…³é”®çš„çŠ¶æ€åˆ¤æ–­å‡½æ•°ï¼Œä»…åœ¨ç”»é¢é•¿æ—¶é—´æ— å˜åŒ–æ—¶è°ƒç”¨
/// æ ¹æ®é…ç½®çš„ LLM åç«¯ç±»å‹è¿›è¡Œåˆ¤æ–­ï¼š
/// - "ollama": ä½¿ç”¨ Ollama æœåŠ¡
/// - "openai": ä½¿ç”¨ OpenAI æˆ–å…¼å®¹æœåŠ¡
/// - "openrouter": ä½¿ç”¨ OpenRouter æœåŠ¡
/// - "none": ä½¿ç”¨ç®€å•çš„å¯å‘å¼åˆ¤æ–­
pub async fn ask_llm_final_status(text: &str, backend: &str, config: &Config) -> Result<TaskStatus, String> {
    if backend == "none" {
        // å¦‚æœç¦ç”¨ LLMï¼Œä½¿ç”¨ç®€å•çš„å¯å‘å¼åˆ¤æ–­
        return Ok(simple_heuristic_check(text));
    }
    
    // åªè¯»å–ä¸€æ¬¡ system prompt
    let system_prompt = include_str!("../prompt_final.md");

    match backend.as_ref() {
        "ollama" => {
            let model = config.llm.ollama.as_ref().map(|o| o.model.clone()).unwrap_or("qwen2.5:3b".to_string());
            let url = config.llm.ollama.as_ref().map(|o| o.url.clone()).unwrap_or("http://localhost:11434".to_string());
            
            // å¯¹äº Ollamaï¼Œæˆ‘ä»¬éœ€è¦å°† system å’Œ user å†…å®¹ä»¥åˆé€‚çš„æ ¼å¼ä¼ é€’
            let ollama_prompt = format!("### ç³»ç»ŸæŒ‡ä»¤\n{}\n\n### ç”¨æˆ·å†…å®¹\n{}", system_prompt, text);
            
            match ask_ollama_with_ollama_rs(&ollama_prompt, &model, &url).await {
                Ok(response) => {
                    let response = response.trim();
                    match response {
                        "DONE" => Ok(TaskStatus::Done),
                        "STUCK" => Ok(TaskStatus::Stuck),
                        _ => Err(format!("LLM è¿”å›æœªçŸ¥çŠ¶æ€: {}", response)),
                    }
                }
                Err(e) => Err(e),
            }
        }
        "openai" => {
            if let Some(openai_config) = &config.llm.openai {
                match ask_openai(&system_prompt, &text, &openai_config).await {
                    Ok(response) => {
                        let response = response.trim();
                        match response {
                            "DONE" => Ok(TaskStatus::Done),
                            "STUCK" => Ok(TaskStatus::Stuck),
                            _ => Err(format!("LLM è¿”å›æœªçŸ¥çŠ¶æ€: {}", response)),
                        }
                    }
                    Err(e) => Err(e),
                }
            } else {
                Err("OpenAI é…ç½®æœªæ‰¾åˆ°".to_string())
            }
        }
        "openrouter" => {
            let url = "https://openrouter.ai/api/v1/chat/completions";
            let model = config.llm.openrouter.as_ref().map(|o| o.model.clone()).unwrap_or("qwen/qwen-2.5-7b-instruct".to_string());
            
            if let Some(openrouter_config) = &config.llm.openrouter {
                let body = json!({
                    "model": model,
                    "messages": [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": text}
                    ],
                    "max_tokens": 4,
                    "temperature": 0.0
                });
                
                match ureq::post(&url)
                    .set("Authorization", &format!("Bearer {}", openrouter_config.api_key))
                    .send_json(body) 
                {
                    Ok(resp) => {
                        let json: Value = resp.into_json().map_err(|e| e.to_string())?;
                        let response = json["choices"][0]["message"]["content"]
                            .as_str()
                            .unwrap_or("")
                            .trim();
                        match response {
                            "DONE" => Ok(TaskStatus::Done),
                            "STUCK" => Ok(TaskStatus::Stuck),
                            _ => Err(format!("LLM è¿”å›æœªçŸ¥çŠ¶æ€: {}", response)),
                        }
                    }
                    Err(e) => Err(format!("OpenRouter è°ƒç”¨å¤±è´¥: {}", e)),
                }
            } else {
                Err("OpenRouter é…ç½®æœªæ‰¾åˆ°".to_string())
            }
        }
        _ => Err("æœªçŸ¥çš„ LLM_BACKEND".to_string()),
    }
}